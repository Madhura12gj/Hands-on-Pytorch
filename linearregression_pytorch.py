# -*- coding: utf-8 -*-
"""LinearRegression_Pytorch

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bCmOY_NG-S62ZfRE3V4Yrjtz9lhhw7D2

Objective of a regression task is to explain and make predictions based on linear relation with an independent variable.  

1. Importing library functions.
"""

import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F

"""2. Defining data points.Creating linear tensors. X_Data is independent variable and Y_data is target variable."""

x_data=Variable(torch.Tensor([[10.0],[9.0],[3.0],[2.0]]))
y_data=Variable(torch.Tensor([[90.0],[80.0],[50.0],[30.0]]))

"""3. Since the model takes one independent variable ans makes one prediction for Y variable at a time, we initialize our model with linear layer as " torch.nn.Linear(1,1) "

4. Defining forward pass function.the function takes X as input and outputs the predicted value of Y i.e.(y_pred)
"""

class LinearRegression (nn.Module):

  def __init__(self):
    super(LinearRegression,self). __init__ ()
    self.linear = torch.nn.Linear(1,1)

  def forward(self, x):
    y_pred = self.linear(x)
    return y_pred

model = LinearRegression()

"""5.A Loss is calculated from y_data and the predicted value y_pred in order to update weights.

Here we use MeanSquaredError which is most commonly used regression loss function.
"""

criterion = torch.nn.MSELoss(size_average=False)

"""6.Stochastic Gradient Descent is used for updating the hyperparameters. model.paramters() provides learnable parameters and optimizer. lr=0.01 defines the learning rates for parameter updates."""

optimizer = torch.optim.SGD(model.parameters(),lr=0.01)

"""7.Epoch is a single pass through whole training dataset.
Here ther will be 20 passes of Training and weight updates.

8.Backward pass - learning and updating of weights. 

8.1 optimizer.zero_grad() set equal to zero, because everytime a variable is back propagated through,the gradient is accumalated insead of being replaced.

8.2 optimizer.step() performs a parameter update based on current gradient.
"""

for epoch in range (20):
  model.train()
  optimizer.zero_grad()

  #forwardpass
  y_pred = model(x_data)

  #ComputeLoss
  loss=criterion(y_pred,y_data)

  #BackwardPass
  loss.backward()
  optimizer.step()

"""9. Since out model is trained,here we add a new variable (new_x) to test our model."""

#make new predictions
new_X = Variable(torch.Tensor([[4.0]]))
y_pred = model(new_X)
print("predicted Y value: ", y_pred.data[0][0])

